{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c626c089-49a6-4ba5-aaee-b2ca5c474fe9",
   "metadata": {},
   "source": [
    "# Read Dataset File of Flickr8K "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a1457c9-30e0-41ca-aa56-3f01446d6917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "3  1000268201_693b08cb0e.jpg   \n",
       "4  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                             caption  \n",
       "0  A child in a pink dress is climbing up a set o...  \n",
       "1              A girl going into a wooden building .  \n",
       "2   A little girl climbing into a wooden playhouse .  \n",
       "3  A little girl climbing the stairs to her playh...  \n",
       "4  A little girl in a pink dress going into a woo...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load captions dataset\n",
    "csv_path = \"G:/multimodal_ai/datasets/captions.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b628ab3-39cc-4870-bb2c-a96869d0fc9c",
   "metadata": {},
   "source": [
    "# Test for Image feature extraction with open_clip model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82666698-4f15-4e74-b244-18a987d1c076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\KARTIK\\.conda\\envs\\multimodal_ai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "D:\\Users\\KARTIK\\.conda\\envs\\multimodal_ai\\lib\\site-packages\\open_clip\\factory.py:380: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Shape: (1, 512)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import open_clip\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Load CLIP model\n",
    "model, preprocess, _ = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"openai\")\n",
    "\n",
    "def extract_image_features(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = preprocess(image).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        features = model.encode_image(image)\n",
    "    return features.cpu().numpy()\n",
    "\n",
    "# Example usage\n",
    "image_path = \"G:/multimodal_ai/datasets/Images/1000268201_693b08cb0e.jpg\"\n",
    "features = extract_image_features(image_path)\n",
    "print(\"Feature Shape:\", features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915e4b95-4655-423d-9046-9b650894b6e2",
   "metadata": {},
   "source": [
    "# Test for Text feature extraction with open_clip model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd509016-98de-469b-af76-2b3d90f922e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Feature Shape: (1, 512)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
    "\n",
    "def extract_text_features(text):\n",
    "    tokens = tokenizer([text])\n",
    "    with torch.no_grad():\n",
    "        features = model.encode_text(tokens)\n",
    "    return features.cpu().numpy()\n",
    "\n",
    "# Example usage\n",
    "caption = \"A child in a pink dress is climbing up a set of stairs\"\n",
    "text_features = extract_text_features(caption)\n",
    "print(\"Text Feature Shape:\", text_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef409b2-de23-40e9-b306-df5dc1cd3a22",
   "metadata": {},
   "source": [
    "# Test FAISS for storing and loading the image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a64c60b-f836-4e1a-8571-9f5e28945827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS Index Size: 1\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "# Create FAISS index\n",
    "index = faiss.IndexFlatL2(512)  # 512-D embedding space\n",
    "\n",
    "def add_to_index(features):\n",
    "    index.add(np.array(features))\n",
    "\n",
    "# Example usage\n",
    "add_to_index(features)  # Add image features\n",
    "print(\"FAISS Index Size:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceb0b0e6-b445-4f24-975b-80d9e0324a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded FAISS Index Size: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Define path\n",
    "faiss_index_path = \"G:/multimodal_ai/models/faiss_index.idx\"\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(faiss_index_path), exist_ok=True)\n",
    "faiss.write_index(index, \"G:/multimodal_ai/models/faiss_index.idx\")\n",
    "\n",
    "# To load later\n",
    "index = faiss.read_index(\"G:/multimodal_ai/models/faiss_index.idx\")\n",
    "print(\"Loaded FAISS Index Size:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f902cd11-275b-4d3d-8653-7102987a7118",
   "metadata": {},
   "source": [
    "# Merge Flickr8k and Flickr30k dataset and Merge all 5 captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4bd0532f-ae7b-42e6-88ee-cbd87ca4582a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final merged dataset saved at: G:/multimodal_ai/datasets/final_captions.csv\n",
      "Total Unique Images: 39874\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "captions8k_path = \"G:/multimodal_ai/datasets/captions8k.csv\"\n",
    "captions30k_path = \"G:/multimodal_ai/datasets/captions30k.csv\"\n",
    "final_csv_path = \"G:/multimodal_ai/datasets/final_captions.csv\"\n",
    "\n",
    "# Load both datasets\n",
    "df_8k = pd.read_csv(captions8k_path)\n",
    "df_30k = pd.read_csv(captions30k_path)\n",
    "\n",
    "# Ensure both datasets have the same column names\n",
    "df_8k = df_8k.rename(columns={\"filename\": \"image\"})\n",
    "df_30k = df_30k.rename(columns={\"filename\": \"image\"})\n",
    "\n",
    "# Combine both datasets\n",
    "df_combined = pd.concat([df_8k, df_30k], ignore_index=True)\n",
    "\n",
    "# **Fix: Ensure all captions are strings and handle NaN values**\n",
    "df_combined[\"caption\"] = df_combined[\"caption\"].astype(str).fillna(\"\")\n",
    "\n",
    "# Merge all captions for each unique image\n",
    "df_final = df_combined.groupby(\"image\")[\"caption\"].apply(lambda x: \"\".join(x)).reset_index()\n",
    "\n",
    "# Save the final dataset\n",
    "df_final.to_csv(final_csv_path, index=False)\n",
    "\n",
    "print(f\"✅ Final merged dataset saved at: {final_csv_path}\")\n",
    "print(f\"Total Unique Images: {df_final['image'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67c46d4a-3fdb-41b5-bb02-7acdb57d82e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>Two young guys with shaggy hair look at their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10002456.jpg</td>\n",
       "      <td>Several men in hard hats are operating a gian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000344755.jpg</td>\n",
       "      <td>Someone in a blue shirt and hat is standing o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0             1000092795.jpg   \n",
       "1               10002456.jpg   \n",
       "2             1000268201.jpg   \n",
       "3  1000268201_693b08cb0e.jpg   \n",
       "4             1000344755.jpg   \n",
       "\n",
       "                                             caption  \n",
       "0   Two young guys with shaggy hair look at their...  \n",
       "1   Several men in hard hats are operating a gian...  \n",
       "2   A child in a pink dress is climbing up a set ...  \n",
       "3  A child in a pink dress is climbing up a set o...  \n",
       "4   Someone in a blue shirt and hat is standing o...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "886c8bb7-afeb-4f68-943e-921b4790f3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Two young guys with shaggy hair look at their hands while hanging out in the yard . Two young , White males are outside near many bushes . Two men in green shirts are standing in a yard . A man in a blue shirt standing in a garden . Two friends enjoy time spent together .'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.caption.iloc[0] #This is required to avoid duplicate image features getting stored in the faiss index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba070ab-3c95-4373-9327-47578c22fd46",
   "metadata": {},
   "source": [
    "# Complete Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45c88970-a3db-4dee-8c1b-33dc7091a03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded: 39874 image-caption pairs\n",
      "Loading CLIP model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\KARTIK\\.conda\\envs\\multimodal_ai\\lib\\site-packages\\open_clip\\factory.py:380: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP model loaded successfully!\n",
      "Extracting features and indexing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data: 100%|█████████████████████████████████████████████████████████| 39874/39874 [1:52:58<00:00,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction complete! Total embeddings stored: 39874\n",
      "FAISS index saved successfully!\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Load dataset\n",
    "csv_path = \"G:/multimodal_ai/datasets/final_captions.csv\"\n",
    "image_folder = \"G:/multimodal_ai/datasets/Images/\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"Dataset Loaded: {len(df)} image-caption pairs\")\n",
    "\n",
    "# Load CLIP model\n",
    "print(\"Loading CLIP model...\")\n",
    "model, preprocess, _ = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"openai\")\n",
    "tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
    "print(\"CLIP model loaded successfully!\")\n",
    "\n",
    "# Create FAISS index\n",
    "index = faiss.IndexFlatL2(512)  # 512-D embedding space\n",
    "\n",
    "# Function to extract image features\n",
    "def extract_image_features(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = preprocess(image).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        features = model.encode_image(image)\n",
    "    return features.cpu().numpy()\n",
    "\n",
    "# Function to extract text features\n",
    "def extract_text_features(text):\n",
    "    tokens = tokenizer([text])\n",
    "    with torch.no_grad():\n",
    "        features = model.encode_text(tokens)\n",
    "    return features.cpu().numpy()\n",
    "\n",
    "# Batch process images & captions\n",
    "print(\"Extracting features and indexing data...\")\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Data\"):\n",
    "    image_path = os.path.join(image_folder, row['image'])\n",
    "    if os.path.exists(image_path):\n",
    "        image_features = extract_image_features(image_path)  # (1, 512)\n",
    "        text_features = extract_text_features(row['caption'])  # (1, 512)\n",
    "        \n",
    "        # Combine features properly\n",
    "        combined_features = (image_features + text_features) / 2  # (1, 512)\n",
    "        combined_features = np.array(combined_features, dtype=np.float32).reshape(1, -1)\n",
    "        \n",
    "        # Add to FAISS\n",
    "        index.add(combined_features)\n",
    "\n",
    "print(f\"Feature extraction complete! Total embeddings stored: {index.ntotal}\")\n",
    "\n",
    "# Save FAISS index\n",
    "faiss_index_path = \"G:/multimodal_ai/models/faiss_index.idx\"\n",
    "os.makedirs(os.path.dirname(faiss_index_path), exist_ok=True)\n",
    "faiss.write_index(index, faiss_index_path)\n",
    "print(\"FAISS index saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55bf93e5-b471-42d1-a18e-e77a9e90bad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURE EXTRACTION COMPLETED!\n"
     ]
    }
   ],
   "source": [
    "print(\"FEATURE EXTRACTION COMPLETED!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1115bed-7a96-4600-b430-0f6acd96ff75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (multimodal_ai)",
   "language": "python",
   "name": "multimodal_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
